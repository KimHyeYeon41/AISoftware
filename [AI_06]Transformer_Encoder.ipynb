{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[AI_06]Transformer_Encoder_201802083.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3ec22df309e542148944b43db673429e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_224eea70aefa4a13a54d4a7e4fddea79",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0479f0b1e11f461c8171405d652a89d1",
              "IPY_MODEL_13fc1faf98474dd083a3da4549d97a03",
              "IPY_MODEL_eb48c7fad32241348fa1c2522305e22d"
            ]
          }
        },
        "224eea70aefa4a13a54d4a7e4fddea79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0479f0b1e11f461c8171405d652a89d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_846361c4c070405ca3504bd629bf8f58",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d97f6bc673d04472bdd6f2a02ea6a8b1"
          }
        },
        "13fc1faf98474dd083a3da4549d97a03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4e2f5c82e37a402484930a733f273ccb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 213450,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 213450,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b9c5b4ae0ba245acb55d959367c7354f"
          }
        },
        "eb48c7fad32241348fa1c2522305e22d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_410e1f64b1d646d9bd1ca5e4017a969b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 208k/208k [00:00&lt;00:00, 2.32MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d7a1220c9b6d495aabaf86aaad102990"
          }
        },
        "846361c4c070405ca3504bd629bf8f58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d97f6bc673d04472bdd6f2a02ea6a8b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4e2f5c82e37a402484930a733f273ccb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b9c5b4ae0ba245acb55d959367c7354f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "410e1f64b1d646d9bd1ca5e4017a969b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d7a1220c9b6d495aabaf86aaad102990": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fbebb35243c44ad4949449664e5db100": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c6e7fbb173cb4d02b0edede6f4276610",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3d7c5206bcc24a51beb51c4cebe43ace",
              "IPY_MODEL_91e404b7eab14775b92a08740f2ff151",
              "IPY_MODEL_f8a18f6bb8504fd390d076ca22f18e60"
            ]
          }
        },
        "c6e7fbb173cb4d02b0edede6f4276610": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3d7c5206bcc24a51beb51c4cebe43ace": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c5f66523570046ec9bad323468884b56",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b00ba805d5f74fe98fa7f7fce44668c6"
          }
        },
        "91e404b7eab14775b92a08740f2ff151": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6f77a37da6b544888f4e8ab8a94caf97",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 29,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 29,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cc5281c725b74b46b0782b8221b7afd0"
          }
        },
        "f8a18f6bb8504fd390d076ca22f18e60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c0db08aa9c1f4c2ba9f0d7877bbd1300",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 29.0/29.0 [00:00&lt;00:00, 480B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5a77605af83943dd9e58bc17f74b7bea"
          }
        },
        "c5f66523570046ec9bad323468884b56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b00ba805d5f74fe98fa7f7fce44668c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6f77a37da6b544888f4e8ab8a94caf97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cc5281c725b74b46b0782b8221b7afd0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c0db08aa9c1f4c2ba9f0d7877bbd1300": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5a77605af83943dd9e58bc17f74b7bea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9789c1c9ddbc41aea149a115494a1cf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7da8dd31ac2f4378801b10dd366431ba",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6029121cd8514ae98ad7562d68802719",
              "IPY_MODEL_5c262bea83d94e3da9ed2ad0f71c9d6e",
              "IPY_MODEL_3f9257c4bdd943dc971719db2f321b7b"
            ]
          }
        },
        "7da8dd31ac2f4378801b10dd366431ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6029121cd8514ae98ad7562d68802719": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_012185c30265495d96f81d65f51329f3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e59b671dbc33460fb2b0d0b3f52a4886"
          }
        },
        "5c262bea83d94e3da9ed2ad0f71c9d6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8f33f21ab4664f60b78360ecf3ab6bc9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 435797,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 435797,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_070fa31902234ebdbeaf8647bfd97a2d"
          }
        },
        "3f9257c4bdd943dc971719db2f321b7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d5c6b48674f040c6bf187109fa02bdbc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 426k/426k [00:00&lt;00:00, 1.70MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4808cef2e10d49ed8f376a2a9671bdac"
          }
        },
        "012185c30265495d96f81d65f51329f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e59b671dbc33460fb2b0d0b3f52a4886": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8f33f21ab4664f60b78360ecf3ab6bc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "070fa31902234ebdbeaf8647bfd97a2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d5c6b48674f040c6bf187109fa02bdbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4808cef2e10d49ed8f376a2a9671bdac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f56772326a3d4dad89aa11ce1c989b54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f9a96bd7c7b149b3a3271d21a2dca4af",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6dca615af8a44376b41cdfae9ce5d52a",
              "IPY_MODEL_b1f6cecd22f34b01823a60fda1ca296e",
              "IPY_MODEL_7e6fd617a1504734985468ee7e06a671"
            ]
          }
        },
        "f9a96bd7c7b149b3a3271d21a2dca4af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6dca615af8a44376b41cdfae9ce5d52a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a17d321fb446439586355e220b7eb39d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d66b48581ab54fd79eefa46a3fe16e4e"
          }
        },
        "b1f6cecd22f34b01823a60fda1ca296e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a5af8b35d7134e7b8e209e4663a6a29e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 570,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 570,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b4c166b481bc4bcab56a9f6861d453f1"
          }
        },
        "7e6fd617a1504734985468ee7e06a671": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6185b135430e4c03be7b396f8d8f2f1f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 570/570 [00:00&lt;00:00, 13.9kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5232a40a42b540f6b07e93dab87b4ad4"
          }
        },
        "a17d321fb446439586355e220b7eb39d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d66b48581ab54fd79eefa46a3fe16e4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a5af8b35d7134e7b8e209e4663a6a29e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b4c166b481bc4bcab56a9f6861d453f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6185b135430e4c03be7b396f8d8f2f1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5232a40a42b540f6b07e93dab87b4ad4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUN8shWjVCBN"
      },
      "source": [
        "## Colab 환경 구축\n",
        "\n",
        "\n",
        "### 활용 라이브러리 (고정)\n",
        "\n",
        "*   [torch==1.9.0](https://pytorch.org/)\n",
        "*   [transformers](https://pypi.org/project/transformers/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUqul7cOUlyi",
        "outputId": "9f37725d-b766-4593-8037-070aec277674"
      },
      "source": [
        "!pip3 install torch==1.9.0 torchvision torchaudio\n",
        "!pip3 install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.7/dist-packages (1.9.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.10.0+cu111)\n",
            "Collecting torchaudio\n",
            "  Downloading torchaudio-0.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.19.5)\n",
            "  Downloading torchaudio-0.9.1-cp37-cp37m-manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 41.1 MB/s \n",
            "\u001b[?25h  Downloading torchaudio-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 38.5 MB/s \n",
            "\u001b[?25hInstalling collected packages: torchaudio\n",
            "Successfully installed torchaudio-0.9.0\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.12.3-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 5.1 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 42.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.1.1-py3-none-any.whl (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 6.7 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 35.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 41.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.1.1 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIT-0TDnRqXf"
      },
      "source": [
        "### 필요 라이브러리 import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6waoQgKVD7_"
      },
      "source": [
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers import (BertTokenizer, BertConfig, \n",
        "                          apply_chunking_to_forward, set_seed\n",
        "                          )\n",
        "from transformers.modeling_utils import find_pruneable_heads_and_indices, prune_linear_layer\n",
        "from transformers.activations import gelu\n",
        "from transformers.modeling_outputs import BaseModelOutputWithPastAndCrossAttentions\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTWOJHEtRvkU"
      },
      "source": [
        "1. Reproducibility- 재구현을 위한 random_seed 42로 설정\n",
        "\n",
        "```\n",
        "set_seed(42)\n",
        "```\n",
        "\n",
        "\n",
        "2. BERT [tokenizer](https://huggingface.co/transformers/model_doc/bert.html#berttokenizer), [configuration(config)](https://huggingface.co/transformers/model_doc/bert.html#bertconfig) 설정\n",
        "\n",
        "```\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "bert_configuraiton = BertConfig.from_pretrained('bert-base-cased')\n",
        "```\n",
        "\n",
        "\n",
        "3. input sequence 토크나이징\n",
        "\n",
        "```\n",
        "input_texts = ['I love cats!', 'He hates pineapple pizza.']\n",
        "\n",
        "input_sequences = tokenizer(text=input_texts, add_special_tokens=True, padding=True, truncation=True, return_tensors='pt')\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJeFcSriVJGe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "3ec22df309e542148944b43db673429e",
            "224eea70aefa4a13a54d4a7e4fddea79",
            "0479f0b1e11f461c8171405d652a89d1",
            "13fc1faf98474dd083a3da4549d97a03",
            "eb48c7fad32241348fa1c2522305e22d",
            "846361c4c070405ca3504bd629bf8f58",
            "d97f6bc673d04472bdd6f2a02ea6a8b1",
            "4e2f5c82e37a402484930a733f273ccb",
            "b9c5b4ae0ba245acb55d959367c7354f",
            "410e1f64b1d646d9bd1ca5e4017a969b",
            "d7a1220c9b6d495aabaf86aaad102990",
            "fbebb35243c44ad4949449664e5db100",
            "c6e7fbb173cb4d02b0edede6f4276610",
            "3d7c5206bcc24a51beb51c4cebe43ace",
            "91e404b7eab14775b92a08740f2ff151",
            "f8a18f6bb8504fd390d076ca22f18e60",
            "c5f66523570046ec9bad323468884b56",
            "b00ba805d5f74fe98fa7f7fce44668c6",
            "6f77a37da6b544888f4e8ab8a94caf97",
            "cc5281c725b74b46b0782b8221b7afd0",
            "c0db08aa9c1f4c2ba9f0d7877bbd1300",
            "5a77605af83943dd9e58bc17f74b7bea",
            "9789c1c9ddbc41aea149a115494a1cf6",
            "7da8dd31ac2f4378801b10dd366431ba",
            "6029121cd8514ae98ad7562d68802719",
            "5c262bea83d94e3da9ed2ad0f71c9d6e",
            "3f9257c4bdd943dc971719db2f321b7b",
            "012185c30265495d96f81d65f51329f3",
            "e59b671dbc33460fb2b0d0b3f52a4886",
            "8f33f21ab4664f60b78360ecf3ab6bc9",
            "070fa31902234ebdbeaf8647bfd97a2d",
            "d5c6b48674f040c6bf187109fa02bdbc",
            "4808cef2e10d49ed8f376a2a9671bdac",
            "f56772326a3d4dad89aa11ce1c989b54",
            "f9a96bd7c7b149b3a3271d21a2dca4af",
            "6dca615af8a44376b41cdfae9ce5d52a",
            "b1f6cecd22f34b01823a60fda1ca296e",
            "7e6fd617a1504734985468ee7e06a671",
            "a17d321fb446439586355e220b7eb39d",
            "d66b48581ab54fd79eefa46a3fe16e4e",
            "a5af8b35d7134e7b8e209e4663a6a29e",
            "b4c166b481bc4bcab56a9f6861d453f1",
            "6185b135430e4c03be7b396f8d8f2f1f",
            "5232a40a42b540f6b07e93dab87b4ad4"
          ]
        },
        "outputId": "75f459dd-58a3-4ce7-adb0-5c7f5fb51106"
      },
      "source": [
        "# Set seed for reproducibility\n",
        "set_seed(42)\n",
        "\n",
        "# GELU Activation function.\n",
        "ACT2FN = {\"gelu\": gelu}\n",
        "\n",
        "# Define BertLayerNorm.\n",
        "BertLayerNorm = nn.LayerNorm\n",
        "\n",
        "# Create BertTokenizer, Configuration\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "bert_configuraiton = BertConfig.from_pretrained('bert-base-cased')\n",
        "\n",
        "# Create input sequence using tokenizer\n",
        "input_texts = ['I love cats!', 'He hates pineapple pizza.']\n",
        "labels = [1, 0]\n",
        "input_sequences = tokenizer(text=input_texts, add_special_tokens=True, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "# Since input sequences is a dictionary we can also add to labels to it\n",
        "# want to make sure all values at tensors\n",
        "input_sequences.update({'labels':torch.tensor(labels)})\n",
        "# print(input_sequences)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3ec22df309e542148944b43db673429e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fbebb35243c44ad4949449664e5db100",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9789c1c9ddbc41aea149a115494a1cf6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/426k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f56772326a3d4dad89aa11ce1c989b54",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Hrgg4UWR7qU"
      },
      "source": [
        "### Input Sequence에 대해 Embedding 진행 (Huggingface 코드)\n",
        "- 변경 필요 없음, 그대로 사용\n",
        "\n",
        "1. BertEmbedding에 대한 configuration 생성\n",
        "2. bert_embedding block에 대해 forward 수행\n",
        "3. Embedding Output Shape : [batch_size, seq_len, hidden_size(768)]\n",
        "\n",
        "```\n",
        "# Create Bert embedding layer\n",
        "bert_embeddings_block = BertEmbeddings(bert_configuraiton)\n",
        "\n",
        "# Perform a forward pass\n",
        "embedding_output = bert_embeddings_block.forward(input_ids=input_sequences['input_ids'], token_type_ids=input_sequences['token_type_ids'])\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLqKqArjVRk-"
      },
      "source": [
        "class BertEmbeddings(nn.Module):\n",
        "    \"\"\"Construct the embeddings from word, position and token_type embeddings.\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n",
        "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
        "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
        "\n",
        "        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n",
        "        # any TensorFlow checkpoint file\n",
        "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n",
        "        self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))\n",
        "        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
        "\n",
        "    def forward(\n",
        "        self, input_ids=None, token_type_ids=None, position_ids=None, inputs_embeds=None, past_key_values_length=0\n",
        "    ):\n",
        "        # print('============== BertEmbeddings ==============')\n",
        "        if input_ids is not None:\n",
        "            input_shape = input_ids.size()\n",
        "        else:\n",
        "            input_shape = inputs_embeds.size()[:-1]\n",
        "\n",
        "        seq_length = input_shape[1]\n",
        "\n",
        "        if position_ids is None:\n",
        "            position_ids = self.position_ids[:, past_key_values_length : seq_length + past_key_values_length]       \n",
        "        #print('Created Tokens Positions IDs: ', position_ids) # ADDED\n",
        "        \n",
        "\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n",
        "\n",
        "        if inputs_embeds is None:\n",
        "            inputs_embeds = self.word_embeddings(input_ids)\n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
        "\n",
        "        # ADDED\n",
        "        # print('Tokens IDs: ', input_ids.shape)\n",
        "        # print('Tokens Type IDs: ', token_type_ids.shape)\n",
        "        # print('Word Embeddings: ', inputs_embeds.shape)\n",
        "\n",
        "        embeddings = inputs_embeds + token_type_embeddings\n",
        "        if self.position_embedding_type == \"absolute\":\n",
        "            position_embeddings = self.position_embeddings(position_ids)\n",
        "            # print('Position Embeddings: ', position_embeddings.shape) # ADDED\n",
        "\n",
        "            embeddings += position_embeddings\n",
        "\n",
        "        # ADDED\n",
        "        # print('Token Types Embeddings: ', token_type_embeddings.shape)\n",
        "        # print('Sum Up All Embeddings: ', embeddings.shape)\n",
        "\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        # print('Embeddings Layer Nromalization: ', embeddings.shape) # ADDED\n",
        "\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        # print('Embeddings Dropout Layer: ', embeddings.shape) # ADDED\n",
        "        \n",
        "        return embeddings"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTuoLW2rVO0n"
      },
      "source": [
        "def get_extended_attention_mask(attention_mask, input_shape, device):\n",
        "        \"\"\"\n",
        "        Makes broadcastable attention and causal masks so that future and masked tokens are ignored.\n",
        "        Arguments:\n",
        "            attention_mask (:obj:`torch.Tensor`):\n",
        "                Mask with ones indicating tokens to attend to, zeros for tokens to ignore.\n",
        "            input_shape (:obj:`Tuple[int]`):\n",
        "                The shape of the input to the model.\n",
        "            device: (:obj:`torch.device`):\n",
        "                The device of the input to the model.\n",
        "        Returns:\n",
        "            :obj:`torch.Tensor` The extended attention mask, with a the same dtype as :obj:`attention_mask.dtype`.\n",
        "        \"\"\"\n",
        "        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
        "        # ourselves in which case we just need to make it broadcastable to all heads.\n",
        "        if attention_mask.dim() == 3:\n",
        "            extended_attention_mask = attention_mask[:, None, :, :]\n",
        "        elif attention_mask.dim() == 2:\n",
        "            # Provided a padding mask of dimensions [batch_size, seq_length]\n",
        "            # - if the model is a decoder, apply a causal mask in addition to the padding mask\n",
        "            # - if the model is an encoder, make the mask broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
        "            \n",
        "            extended_attention_mask = attention_mask[:, None, None, :]\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"Wrong shape for input_ids (shape {input_shape}) or attention_mask (shape {attention_mask.shape})\"\n",
        "            )\n",
        "\n",
        "        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
        "        # masked positions, this operation will create a tensor which is 0.0 for\n",
        "        # positions we want to attend and -10000.0 for masked positions.\n",
        "        # Since we are adding it to the raw scores before the softmax, this is\n",
        "        # effectively the same as removing these entirely.\n",
        "        \n",
        "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
        "        return extended_attention_mask.to(device)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wS0J8k-qSE-n"
      },
      "source": [
        "### BertSelfAttention (Huggingface 코드)\n",
        "\n",
        "- Self-Attention 수행 \n",
        "\n",
        "- 결과로 나오는 output shape : [batch_size, seq_len, 768] "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1wq0rM-VSuK"
      },
      "source": [
        "class BertSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n",
        "            raise ValueError(\n",
        "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
        "                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads)\n",
        "            )\n",
        "\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        # ADDED\n",
        "        # print('============== BertSelfAttention ==============')\n",
        "        # print('Attention Head Size: ', self.attention_head_size)\n",
        "        # print('Combined Attentions Head Size: ', self.all_head_size)\n",
        "\n",
        "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
        "        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
        "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
        "            self.max_position_embeddings = config.max_position_embeddings\n",
        "            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n",
        "\n",
        "        self.is_decoder = config.is_decoder\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        past_key_value=None,\n",
        "        output_attentions=False,\n",
        "    ):\n",
        "        \n",
        "        # print('Hidden States: ', hidden_states.shape) # ADDED\n",
        "\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "\n",
        "        # If this is instantiated as a cross-attention module, the keys\n",
        "        # and values come from an encoder; the attention mask needs to be\n",
        "        # such that the encoder's padding tokens are not attended to.\n",
        "        is_cross_attention = encoder_hidden_states is not None\n",
        "\n",
        "        if is_cross_attention and past_key_value is not None:\n",
        "            # ADDED\n",
        "            # print('Query Linear Layer: ', mixed_query_layer.shape)\n",
        "            # print('Key Linear Layer: ', past_key_value[0].shape)\n",
        "            # print('Value Linear Layer: ', past_key_value[1].shape)\n",
        "\n",
        "            # reuse k,v, cross_attentions\n",
        "            key_layer = past_key_value[0]\n",
        "            value_layer = past_key_value[1]\n",
        "            attention_mask = encoder_attention_mask\n",
        "        \n",
        "        elif is_cross_attention:\n",
        "            # ADDED\n",
        "            # print('Query Linear Layer: ', mixed_query_layer.shape)\n",
        "            # print('Key Linear Layer: ', self.key(encoder_hidden_states).shape)\n",
        "            # print('Value Linear Layer: ', self.value(encoder_hidden_states).shape)\n",
        "\n",
        "            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n",
        "            attention_mask = encoder_attention_mask\n",
        "        \n",
        "        elif past_key_value is not None:\n",
        "            # ADDED\n",
        "            # print('Query Linear Layer: ', mixed_query_layer.shape)\n",
        "            # print('Key Linear Layer: ', self.key(hidden_states).shape)\n",
        "            # print('Value Linear Layer: ', self.value(hidden_states).shape)\n",
        "\n",
        "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n",
        "            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n",
        "        \n",
        "        else:\n",
        "            # ADDED\n",
        "            # print('Query Linear Layer: ', mixed_query_layer.shape)\n",
        "            # print('Key Linear Layer: ', self.key(hidden_states).shape)\n",
        "            # print('Value Linear Layer: ', self.value(hidden_states).shape)\n",
        "\n",
        "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "\n",
        "        # ADDED\n",
        "        # print('Query: ', query_layer.shape)\n",
        "        # print('Key: ', key_layer.shape)\n",
        "        # print('Value: ', value_layer.shape)\n",
        "\n",
        "        if self.is_decoder:\n",
        "            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n",
        "            # Further calls to cross_attention layer can then reuse all cross-attention\n",
        "            # key/value_states (first \"if\" case)\n",
        "            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n",
        "            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n",
        "            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n",
        "            # if encoder bi-directional self-attention `past_key_value` is always `None`\n",
        "            past_key_value = (key_layer, value_layer)\n",
        "\n",
        "        # ADDED\n",
        "        # print('Key Transposed: ', key_layer.transpose(-1, -2).shape)\n",
        "\n",
        "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "\n",
        "        # ADDED\n",
        "        # print('Attention Scores: ', attention_scores.shape)\n",
        "\n",
        "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
        "            seq_length = hidden_states.size()[1]\n",
        "            position_ids_l = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n",
        "            position_ids_r = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n",
        "            distance = position_ids_l - position_ids_r\n",
        "            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n",
        "            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n",
        "\n",
        "            if self.position_embedding_type == \"relative_key\":\n",
        "                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
        "                attention_scores = attention_scores + relative_position_scores\n",
        "            elif self.position_embedding_type == \"relative_key_query\":\n",
        "                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
        "                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n",
        "                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n",
        "\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size) # root 계산\n",
        "        # print('Attention Scores Divided by Scalar: ', attention_scores.shape) # ADDED\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
        "            attention_scores = attention_scores + attention_mask\n",
        "\n",
        "        # Normalize the attention scores to probabilities.\n",
        "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
        "        # print('Attention Probabilities Softmax Layer: ', attention_probs.shape) # ADDED\n",
        "\n",
        "        # This is actually dropping out entire tokens to attend to, which might\n",
        "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "        # print('Attention Probabilities Dropout Layer: ', attention_probs.shape) # ADDED\n",
        "\n",
        "        # Mask heads if we want to\n",
        "        if head_mask is not None:\n",
        "            attention_probs = attention_probs * head_mask\n",
        "\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "        # print('Context: ', context_layer.shape) # ADDED\n",
        "\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        # print('Context Permute: ', context_layer.shape) # ADDED\n",
        "\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(*new_context_layer_shape)\n",
        "        # print('Context Reshaped: ', context_layer.shape) # ADDED\n",
        "        \n",
        "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
        "\n",
        "        if self.is_decoder:\n",
        "            outputs = outputs + (past_key_value,)\n",
        "        return outputs"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYRYbKy52vqy"
      },
      "source": [
        "### BertSelfOutput (Huggingface 코드)\n",
        "\n",
        "- Add&Norm 수행 \n",
        "\n",
        "- 결과로 나오는 output shape : [batch_size, seq_len, 768] "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UE9bl9AVUEp"
      },
      "source": [
        "class BertSelfOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        # print('Hidden States: ', hidden_states.shape)\n",
        "\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        # print('Hidden States Linear Layer: ', hidden_states.shape)\n",
        "\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        # print('Hidden States Dropout Layer: ', hidden_states.shape)\n",
        "\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        # print('Hidden States Normalization Layer: ', hidden_states.shape)\n",
        "\n",
        "        return hidden_states"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQGl7CWyVVQG"
      },
      "source": [
        "class BertAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.self = BertSelfAttention(config)\n",
        "        self.output = BertSelfOutput(config)\n",
        "        self.pruned_heads = set()\n",
        "\n",
        "    def prune_heads(self, heads):\n",
        "        if len(heads) == 0:\n",
        "            return\n",
        "        heads, index = find_pruneable_heads_and_indices(\n",
        "            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n",
        "        )\n",
        "\n",
        "        # Prune linear layers\n",
        "        self.self.query = prune_linear_layer(self.self.query, index)\n",
        "        self.self.key = prune_linear_layer(self.self.key, index)\n",
        "        self.self.value = prune_linear_layer(self.self.value, index)\n",
        "        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n",
        "\n",
        "        # Update hyper params and store pruned heads\n",
        "        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n",
        "        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n",
        "        self.pruned_heads = self.pruned_heads.union(heads)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        past_key_value=None,\n",
        "        output_attentions=False,\n",
        "    ):\n",
        "        self_outputs = self.self(\n",
        "            hidden_states,\n",
        "            attention_mask,\n",
        "            head_mask,\n",
        "            encoder_hidden_states,\n",
        "            encoder_attention_mask,\n",
        "            past_key_value,\n",
        "            output_attentions,\n",
        "        )\n",
        "        attention_output = self.output(self_outputs[0], hidden_states)\n",
        "        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n",
        "        return outputs"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKxOJM2D3CHX"
      },
      "source": [
        "### BertIntermediate (Huggingface 코드)\n",
        "\n",
        "- Position-wise Feed-Forward Networks\n",
        "\n",
        "- 결과로 나오는 output shape : [batch_size, seq_len, 4*768] "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCTwBN2DVWYv"
      },
      "source": [
        "class BertIntermediate(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "        if isinstance(config.hidden_act, str):\n",
        "            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n",
        "        else:\n",
        "            self.intermediate_act_fn = config.hidden_act\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        # print('Hidden States: ', hidden_states.shape)\n",
        "\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        # print('Hidden States Linear Layer: ', hidden_states.shape)\n",
        "\n",
        "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
        "        # print('Hidden States Gelu Activation Function: ', hidden_states.shape)\n",
        "\n",
        "        return hidden_states"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVOUKj3d3Mtw"
      },
      "source": [
        "### BertOutput (Huggingface 코드)\n",
        "\n",
        "- Add&Norm 수행 \n",
        "\n",
        "- 결과로 나오는 output shape : [batch_size, seq_len, 768] "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vWZhPwcVXjR"
      },
      "source": [
        "class BertOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        # print('Hidden States: ', hidden_states.shape)\n",
        "\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        # print('Hidden States Linear Layer: ', hidden_states.shape)\n",
        "\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        # print('Hidden States Dropout Layer: ', hidden_states.shape)\n",
        "\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        # print('Hidden States Layer Normalization: ', hidden_states.shape)\n",
        "\n",
        "        return hidden_states"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgDwcOshVYtI"
      },
      "source": [
        "class BertLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n",
        "        self.seq_len_dim = 1\n",
        "        self.attention = BertAttention(config)\n",
        "        self.is_decoder = config.is_decoder\n",
        "        self.add_cross_attention = config.add_cross_attention\n",
        "        if self.add_cross_attention:\n",
        "            assert self.is_decoder, f\"{self} should be used as a decoder model if cross attention is added\"\n",
        "            self.crossattention = BertAttention(config)\n",
        "        self.intermediate = BertIntermediate(config)\n",
        "        self.output = BertOutput(config)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        past_key_value=None,\n",
        "        output_attentions=False,\n",
        "    ):\n",
        "        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n",
        "        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n",
        "        self_attention_outputs = self.attention(\n",
        "            hidden_states,\n",
        "            attention_mask,\n",
        "            head_mask,\n",
        "            output_attentions=output_attentions,\n",
        "            past_key_value=self_attn_past_key_value,\n",
        "        )\n",
        "        attention_output = self_attention_outputs[0]\n",
        "\n",
        "        # if decoder, the last output is tuple of self-attn cache\n",
        "        if self.is_decoder:\n",
        "            outputs = self_attention_outputs[1:-1]\n",
        "            present_key_value = self_attention_outputs[-1]\n",
        "        else:\n",
        "            outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n",
        "\n",
        "        cross_attn_present_key_value = None\n",
        "        if self.is_decoder and encoder_hidden_states is not None:\n",
        "            assert hasattr(\n",
        "                self, \"crossattention\"\n",
        "            ), f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers by setting `config.add_cross_attention=True`\"\n",
        "\n",
        "            # cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple\n",
        "            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n",
        "            cross_attention_outputs = self.crossattention(\n",
        "                attention_output,\n",
        "                attention_mask,\n",
        "                head_mask,\n",
        "                encoder_hidden_states,\n",
        "                encoder_attention_mask,\n",
        "                cross_attn_past_key_value,\n",
        "                output_attentions,\n",
        "            )\n",
        "            attention_output = cross_attention_outputs[0]\n",
        "            outputs = outputs + cross_attention_outputs[1:-1]  # add cross attentions if we output attention weights\n",
        "\n",
        "            # add cross-attn cache to positions 3,4 of present_key_value tuple\n",
        "            cross_attn_present_key_value = cross_attention_outputs[-1]\n",
        "            present_key_value = present_key_value + cross_attn_present_key_value\n",
        "\n",
        "        layer_output = apply_chunking_to_forward(\n",
        "            self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n",
        "        )\n",
        "        outputs = (layer_output,) + outputs\n",
        "\n",
        "        # if decoder, return the attn key/values as the last output\n",
        "        if self.is_decoder:\n",
        "            outputs = outputs + (present_key_value,)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def feed_forward_chunk(self, attention_output):\n",
        "        intermediate_output = self.intermediate(attention_output)\n",
        "        layer_output = self.output(intermediate_output, attention_output)\n",
        "        return layer_output"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqDeZwY53U7N"
      },
      "source": [
        "### BertEncoder (Huggingface 코드)\n",
        "\n",
        "- num_hidden_layers 만큼 BertLayer 반복 (BERT의 경우 12개 layer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0BqF2ESVZsN"
      },
      "source": [
        "class BertEncoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)]) # config.num_hidden_layers 만큼 반복\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        past_key_values=None,\n",
        "        use_cache=None,\n",
        "        output_attentions=False,\n",
        "        output_hidden_states=False,\n",
        "        return_dict=True,\n",
        "    ):\n",
        "        all_hidden_states = () if output_hidden_states else None\n",
        "        all_self_attentions = () if output_attentions else None\n",
        "        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n",
        "\n",
        "        next_decoder_cache = () if use_cache else None\n",
        "        for i, layer_module in enumerate(self.layer):\n",
        "\n",
        "            # ADDED\n",
        "            print('----------------- BERT LAYER %d -----------------'%(i+1))\n",
        "\n",
        "            if output_hidden_states:\n",
        "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
        "            past_key_value = past_key_values[i] if past_key_values is not None else None\n",
        "            if getattr(self.config, \"gradient_checkpointing\", False):\n",
        "\n",
        "                def create_custom_forward(module):\n",
        "                    def custom_forward(*inputs):\n",
        "                        return module(*inputs, past_key_value, output_attentions)\n",
        "\n",
        "                    return custom_forward\n",
        "\n",
        "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
        "                    create_custom_forward(layer_module),\n",
        "                    hidden_states,\n",
        "                    attention_mask,\n",
        "                    layer_head_mask,\n",
        "                    encoder_hidden_states,\n",
        "                    encoder_attention_mask,\n",
        "                )\n",
        "            else:\n",
        "                layer_outputs = layer_module(\n",
        "                    hidden_states,\n",
        "                    attention_mask,\n",
        "                    layer_head_mask,\n",
        "                    encoder_hidden_states,\n",
        "                    encoder_attention_mask,\n",
        "                    past_key_value,\n",
        "                    output_attentions,\n",
        "                )\n",
        "\n",
        "            hidden_states = layer_outputs[0]\n",
        "            if use_cache:\n",
        "                next_decoder_cache += (layer_outputs[-1],)\n",
        "            if output_attentions:\n",
        "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
        "                if self.config.add_cross_attention:\n",
        "                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n",
        "\n",
        "        if output_hidden_states:\n",
        "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "        if not return_dict:\n",
        "            return tuple(\n",
        "                v\n",
        "                for v in [\n",
        "                    hidden_states,\n",
        "                    next_decoder_cache,\n",
        "                    all_hidden_states,\n",
        "                    all_self_attentions,\n",
        "                    all_cross_attentions,\n",
        "                ]\n",
        "                if v is not None\n",
        "            )\n",
        "        return BaseModelOutputWithPastAndCrossAttentions(\n",
        "            last_hidden_state=hidden_states,\n",
        "            past_key_values=next_decoder_cache,\n",
        "            hidden_states=all_hidden_states,\n",
        "            attentions=all_self_attentions,\n",
        "            cross_attentions=all_cross_attentions,\n",
        "        )"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Wli5njfVan0",
        "outputId": "39d1547d-5709-41b4-961f-4566f46376ea"
      },
      "source": [
        "# -- 1. Bert Embedding (huggingface) -- #\n",
        "bert_embeddings_block = BertEmbeddings(bert_configuraiton)\n",
        "\n",
        "# -- 2. Bert Encoder (huggingface) -- #\n",
        "bert_encoder_block = BertEncoder(bert_configuraiton)\n",
        "bert_encoder_block.eval()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertEncoder(\n",
              "  (layer): ModuleList(\n",
              "    (0): BertLayer(\n",
              "      (attention): BertAttention(\n",
              "        (self): BertSelfAttention(\n",
              "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (output): BertSelfOutput(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (intermediate): BertIntermediate(\n",
              "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "      )\n",
              "      (output): BertOutput(\n",
              "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (1): BertLayer(\n",
              "      (attention): BertAttention(\n",
              "        (self): BertSelfAttention(\n",
              "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (output): BertSelfOutput(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (intermediate): BertIntermediate(\n",
              "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "      )\n",
              "      (output): BertOutput(\n",
              "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (2): BertLayer(\n",
              "      (attention): BertAttention(\n",
              "        (self): BertSelfAttention(\n",
              "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (output): BertSelfOutput(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (intermediate): BertIntermediate(\n",
              "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "      )\n",
              "      (output): BertOutput(\n",
              "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (3): BertLayer(\n",
              "      (attention): BertAttention(\n",
              "        (self): BertSelfAttention(\n",
              "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (output): BertSelfOutput(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (intermediate): BertIntermediate(\n",
              "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "      )\n",
              "      (output): BertOutput(\n",
              "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (4): BertLayer(\n",
              "      (attention): BertAttention(\n",
              "        (self): BertSelfAttention(\n",
              "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (output): BertSelfOutput(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (intermediate): BertIntermediate(\n",
              "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "      )\n",
              "      (output): BertOutput(\n",
              "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (5): BertLayer(\n",
              "      (attention): BertAttention(\n",
              "        (self): BertSelfAttention(\n",
              "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (output): BertSelfOutput(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (intermediate): BertIntermediate(\n",
              "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "      )\n",
              "      (output): BertOutput(\n",
              "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (6): BertLayer(\n",
              "      (attention): BertAttention(\n",
              "        (self): BertSelfAttention(\n",
              "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (output): BertSelfOutput(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (intermediate): BertIntermediate(\n",
              "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "      )\n",
              "      (output): BertOutput(\n",
              "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (7): BertLayer(\n",
              "      (attention): BertAttention(\n",
              "        (self): BertSelfAttention(\n",
              "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (output): BertSelfOutput(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (intermediate): BertIntermediate(\n",
              "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "      )\n",
              "      (output): BertOutput(\n",
              "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (8): BertLayer(\n",
              "      (attention): BertAttention(\n",
              "        (self): BertSelfAttention(\n",
              "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (output): BertSelfOutput(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (intermediate): BertIntermediate(\n",
              "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "      )\n",
              "      (output): BertOutput(\n",
              "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (9): BertLayer(\n",
              "      (attention): BertAttention(\n",
              "        (self): BertSelfAttention(\n",
              "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (output): BertSelfOutput(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (intermediate): BertIntermediate(\n",
              "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "      )\n",
              "      (output): BertOutput(\n",
              "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (10): BertLayer(\n",
              "      (attention): BertAttention(\n",
              "        (self): BertSelfAttention(\n",
              "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (output): BertSelfOutput(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (intermediate): BertIntermediate(\n",
              "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "      )\n",
              "      (output): BertOutput(\n",
              "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (11): BertLayer(\n",
              "      (attention): BertAttention(\n",
              "        (self): BertSelfAttention(\n",
              "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (output): BertSelfOutput(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (intermediate): BertIntermediate(\n",
              "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "      )\n",
              "      (output): BertOutput(\n",
              "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfggskcATHbA"
      },
      "source": [
        "### Custom Transformer Encoder\n",
        "\n",
        "- Encoder\n",
        "  - Encoder Layer * 12\n",
        "    - MultiHeadAttention : Self-Attention\n",
        "    - FeedForward : Huggingface의 Bert Intermediate와 같은 기능, feedforward & GELU 수행"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDhtPTOQVb0h"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from transformers.activations import gelu\n",
        "import math\n",
        "\n",
        "\"\"\" \n",
        "BERT base config\n",
        "hidden_size = 768\n",
        "num_attention_heads = 12\n",
        "num_hidden_layers = 12\n",
        "hidden_dropout_prob = 0.1\n",
        "hidden_act = gelu\n",
        "\"\"\"\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()      \n",
        "        assert config.hidden_size % config.num_attention_heads == 0\n",
        "        \n",
        "        self.hidden_size = config.hidden_size # 768\n",
        "        self.num_attention_heads = config.num_attention_heads # 12\n",
        "        self.head_dim = config.hidden_size // config.num_attention_heads \n",
        "        \n",
        "        self.query = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "        self.key = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "        self.value = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
        "        \n",
        "    \n",
        "    def forward(self, hidden_states, attention_mask=None):\n",
        "      \n",
        "        print('Hidden States: ', hidden_states.shape) # ADDED\n",
        "\n",
        "        Q = self.query(hidden_states)\n",
        "        K = self.key(hidden_states)\n",
        "        V = self.value(hidden_states)\n",
        "        print('Q.size',Q.size())\n",
        "        print('K.size',K.size())\n",
        "        print('V.size',V.size())\n",
        "       \n",
        "        batch_size = hidden_states.shape[0]\n",
        "               \n",
        "        Q = Q.view(batch_size, -1, self.num_attention_heads, self.head_dim).permute(0,2,1,3)\n",
        "        K = K.view(batch_size, -1, self.num_attention_heads, self.head_dim).permute(0,2,1,3)\n",
        "        V = V.view(batch_size, -1, self.num_attention_heads, self.head_dim).permute(0,2,1,3)\n",
        "        print('Q.size',Q.size())\n",
        "        print('K.size',K.size())\n",
        "        print('V.size',V.size())\n",
        "        \n",
        "        d_k = self.head_dim # d_k\n",
        "        print('dk',d_k)\n",
        "        print('transpose k', K.transpose(-2,-1).size())\n",
        "        attention_score = torch.matmul(Q, K.transpose(-1,-2)) # Q x K^T\n",
        "        attention_score = attention_score / math.sqrt(d_k) \n",
        "        print('attention score: ', attention_score.size())\n",
        "        \n",
        "        if attention_mask is not None:\n",
        "          attention_score = attention_score + attention_mask\n",
        "        \n",
        "        attention = nn.functional.softmax(attention_score, dim=-1) \n",
        "        print('softmax attention score: ', attention.size())\n",
        "        \n",
        "        attention = self.dropout(attention)\n",
        "        \n",
        "        output = torch.matmul(attention,V) \n",
        "        print('score*v',output.size())\n",
        "\n",
        "        output = output.permute(0, 2, 1, 3) \n",
        "        print('permute output',output.size())\n",
        "\n",
        "        output = output.reshape(2,9,768)\n",
        "        print('reshape output: ', output.size())\n",
        "\n",
        "        return output\n",
        "        \n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.linear = nn.Linear(config.hidden_size, config.intermediate_size) # 768 -> 3072\n",
        "\n",
        "        self.conv1 = nn.Conv1d(config.hidden_size,config.intermediate_size,1)\n",
        "        self.conv2 = nn.Conv2d(config.intermediate_size,config.hidden_size,1)\n",
        "        self.activate = nn.functional.gelu\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "                \n",
        "    def forward(self, hidden_states):\n",
        "        # hidden_states = [batch size, seq len, hid dim]\n",
        "        \n",
        "        # linear -> gelu\n",
        "        ### Custom Code 작성 ###      \n",
        "        print('hidden_states(input)',hidden_states.size())\n",
        "        #output = self.conv1(hidden_states.transpose(1,2))\n",
        "        output = self.linear(hidden_states)\n",
        "        print('linear output',output.size())\n",
        "        output = self.activate(output)\n",
        "        print('activate output',output.size())\n",
        "        #output = self.conv2(output).transpse(1,2)\n",
        "        output = self.dropout(output)\n",
        "        print('dropout output',output.size())\n",
        "        \n",
        "        return output\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.self_attention = MultiHeadAttention(config)\n",
        "        self.feedforward = FeedForward(config)  \n",
        "\n",
        "        self.linear_1 = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.linear_2= nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        \n",
        "    def forward(self, hidden_states, attention_mask=None):\n",
        "        # hidden_states = [batch_size, seq_len, hidden_size]\n",
        "                \n",
        "        # 1. multi-head attention\n",
        "        attention_output = self.self_attention(hidden_states, attention_mask)\n",
        "\n",
        "        # 2. add & norm : linear -> dropout -> residual connection and layer norm\n",
        "        ### Custom Code 작성 ### \n",
        "        attention_output = self.linear_1(attention_output)\n",
        "        attention_output = self.dropout(attention_output)\n",
        "        attention_output = self.layer_norm(hidden_states + attention_output)\n",
        "        print('attention_output',attention_output.size())\n",
        "\n",
        "        # 3. feedforward\n",
        "        feedforward_output = self.feedforward(attention_output)\n",
        "\n",
        "        # 4. add & norm\n",
        "        ### Custom Code 작성 ###\n",
        "        feedforward_output = self.linear_2(feedforward_output)\n",
        "        feedforward_output = self.dropout(feedforward_output)\n",
        "        feedforward_output = self.layer_norm(feedforward_output + attention_output) \n",
        "        print('feedforward_output',feedforward_output.size())\n",
        "\n",
        "        return feedforward_output\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.config = config\n",
        "        self.layer = nn.ModuleList([EncoderLayer(config) for _ in range(config.num_hidden_layers)]) # layer 만큼 반복하여 생성\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask=None):\n",
        "        \n",
        "        for layer in self.layer:\n",
        "            hidden_states = layer(hidden_states, attention_mask)\n",
        "            \n",
        "        return hidden_states"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKP4UKwPUaF2"
      },
      "source": [
        "- Encoder 선언\n",
        "- Huggingface BertEncoder 파라미터를 custom encoder로 가져옴 (parameter copy)\n",
        "- Randomness 최소화\n",
        "  - eval() 모드\n",
        "  - with torch.no_grad() \n",
        "\n",
        "- 두 개의 Encoder로부터 Forward 결과로 나오는 output 값 비교"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnV5SVzeVgF6",
        "outputId": "710c6e50-51ee-47fd-dc37-d835652aff94"
      },
      "source": [
        "# -- custom transformer encoder -- #\n",
        "custom_encoder = Encoder(bert_configuraiton)\n",
        "\n",
        "# -- Parameter Copying -- #\n",
        "# -- huggingface transformers의 parameter를 custom encoder로 복사 -- #\n",
        "for layer_num, enc_layer in enumerate(bert_encoder_block.layer):\n",
        "    # <<< to MultiHeadAttention (wq, wk, wv) >>>\n",
        "    custom_encoder.layer[layer_num].self_attention.query.load_state_dict(enc_layer.attention.self.query.state_dict()) # wq\n",
        "    custom_encoder.layer[layer_num].self_attention.key.load_state_dict(enc_layer.attention.self.key.state_dict()) # wk\n",
        "    custom_encoder.layer[layer_num].self_attention.value.load_state_dict(enc_layer.attention.self.value.state_dict()) # wv\n",
        "\n",
        "    # <<< to position-wise feedforward (feed_forward_1, feed_forward_2) >>>\n",
        "    custom_encoder.layer[layer_num].linear_1.load_state_dict(enc_layer.attention.output.dense.state_dict()) # feed_forward_1\n",
        "    custom_encoder.layer[layer_num].feedforward.linear.load_state_dict(enc_layer.intermediate.dense.state_dict()) # feed_forward_2\n",
        "    custom_encoder.layer[layer_num].linear_2.load_state_dict(enc_layer.output.dense.state_dict()) # feed_forward_3\n",
        "\n",
        "# eval mode 설정\n",
        "custom_encoder.eval()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Encoder(\n",
              "  (layer): ModuleList(\n",
              "    (0): EncoderLayer(\n",
              "      (self_attention): MultiHeadAttention(\n",
              "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (feedforward): FeedForward(\n",
              "        (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (conv1): Conv1d(768, 3072, kernel_size=(1,), stride=(1,))\n",
              "        (conv2): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (linear_1): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    )\n",
              "    (1): EncoderLayer(\n",
              "      (self_attention): MultiHeadAttention(\n",
              "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (feedforward): FeedForward(\n",
              "        (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (conv1): Conv1d(768, 3072, kernel_size=(1,), stride=(1,))\n",
              "        (conv2): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (linear_1): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    )\n",
              "    (2): EncoderLayer(\n",
              "      (self_attention): MultiHeadAttention(\n",
              "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (feedforward): FeedForward(\n",
              "        (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (conv1): Conv1d(768, 3072, kernel_size=(1,), stride=(1,))\n",
              "        (conv2): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (linear_1): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    )\n",
              "    (3): EncoderLayer(\n",
              "      (self_attention): MultiHeadAttention(\n",
              "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (feedforward): FeedForward(\n",
              "        (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (conv1): Conv1d(768, 3072, kernel_size=(1,), stride=(1,))\n",
              "        (conv2): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (linear_1): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    )\n",
              "    (4): EncoderLayer(\n",
              "      (self_attention): MultiHeadAttention(\n",
              "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (feedforward): FeedForward(\n",
              "        (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (conv1): Conv1d(768, 3072, kernel_size=(1,), stride=(1,))\n",
              "        (conv2): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (linear_1): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    )\n",
              "    (5): EncoderLayer(\n",
              "      (self_attention): MultiHeadAttention(\n",
              "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (feedforward): FeedForward(\n",
              "        (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (conv1): Conv1d(768, 3072, kernel_size=(1,), stride=(1,))\n",
              "        (conv2): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (linear_1): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    )\n",
              "    (6): EncoderLayer(\n",
              "      (self_attention): MultiHeadAttention(\n",
              "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (feedforward): FeedForward(\n",
              "        (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (conv1): Conv1d(768, 3072, kernel_size=(1,), stride=(1,))\n",
              "        (conv2): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (linear_1): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    )\n",
              "    (7): EncoderLayer(\n",
              "      (self_attention): MultiHeadAttention(\n",
              "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (feedforward): FeedForward(\n",
              "        (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (conv1): Conv1d(768, 3072, kernel_size=(1,), stride=(1,))\n",
              "        (conv2): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (linear_1): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    )\n",
              "    (8): EncoderLayer(\n",
              "      (self_attention): MultiHeadAttention(\n",
              "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (feedforward): FeedForward(\n",
              "        (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (conv1): Conv1d(768, 3072, kernel_size=(1,), stride=(1,))\n",
              "        (conv2): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (linear_1): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    )\n",
              "    (9): EncoderLayer(\n",
              "      (self_attention): MultiHeadAttention(\n",
              "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (feedforward): FeedForward(\n",
              "        (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (conv1): Conv1d(768, 3072, kernel_size=(1,), stride=(1,))\n",
              "        (conv2): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (linear_1): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    )\n",
              "    (10): EncoderLayer(\n",
              "      (self_attention): MultiHeadAttention(\n",
              "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (feedforward): FeedForward(\n",
              "        (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (conv1): Conv1d(768, 3072, kernel_size=(1,), stride=(1,))\n",
              "        (conv2): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (linear_1): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    )\n",
              "    (11): EncoderLayer(\n",
              "      (self_attention): MultiHeadAttention(\n",
              "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (feedforward): FeedForward(\n",
              "        (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (conv1): Conv1d(768, 3072, kernel_size=(1,), stride=(1,))\n",
              "        (conv2): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (linear_1): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbDztxRXViXN",
        "outputId": "83ddb8fa-480b-4652-9d3b-b2a39a0afdf9"
      },
      "source": [
        "with torch.no_grad():\n",
        "    input_shape= input_sequences.input_ids.shape\n",
        "    attention_mask = get_extended_attention_mask(input_sequences.attention_mask, input_shape, input_sequences.input_ids.device)\n",
        "\n",
        "    # [Huggingface] Perform a forward pass\n",
        "    embedding_output = bert_embeddings_block.forward(input_ids=input_sequences['input_ids'], token_type_ids=input_sequences['token_type_ids'])\n",
        "    encoder_embedding = bert_encoder_block.forward(hidden_states=embedding_output, attention_mask=attention_mask)\n",
        "\n",
        "    # [Custom] Perform a forward pass\n",
        "    layer_output_custom = custom_encoder.forward(hidden_states=embedding_output, attention_mask=attention_mask)\n",
        "\n",
        "print('[Huggingface] Output : ', encoder_embedding)\n",
        "print('[Custom] Output : ', layer_output_custom)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------- BERT LAYER 1 -----------------\n",
            "----------------- BERT LAYER 2 -----------------\n",
            "----------------- BERT LAYER 3 -----------------\n",
            "----------------- BERT LAYER 4 -----------------\n",
            "----------------- BERT LAYER 5 -----------------\n",
            "----------------- BERT LAYER 6 -----------------\n",
            "----------------- BERT LAYER 7 -----------------\n",
            "----------------- BERT LAYER 8 -----------------\n",
            "----------------- BERT LAYER 9 -----------------\n",
            "----------------- BERT LAYER 10 -----------------\n",
            "----------------- BERT LAYER 11 -----------------\n",
            "----------------- BERT LAYER 12 -----------------\n",
            "Hidden States:  torch.Size([2, 9, 768])\n",
            "Q.size torch.Size([2, 9, 768])\n",
            "K.size torch.Size([2, 9, 768])\n",
            "V.size torch.Size([2, 9, 768])\n",
            "Q.size torch.Size([2, 12, 9, 64])\n",
            "K.size torch.Size([2, 12, 9, 64])\n",
            "V.size torch.Size([2, 12, 9, 64])\n",
            "dk 64\n",
            "transpose k torch.Size([2, 12, 64, 9])\n",
            "attention score:  torch.Size([2, 12, 9, 9])\n",
            "softmax attention score:  torch.Size([2, 12, 9, 9])\n",
            "score*v torch.Size([2, 12, 9, 64])\n",
            "permute output torch.Size([2, 9, 12, 64])\n",
            "reshape output:  torch.Size([2, 9, 768])\n",
            "attention_output torch.Size([2, 9, 768])\n",
            "hidden_states(input) torch.Size([2, 9, 768])\n",
            "linear output torch.Size([2, 9, 3072])\n",
            "activate output torch.Size([2, 9, 3072])\n",
            "dropout output torch.Size([2, 9, 3072])\n",
            "feedforward_output torch.Size([2, 9, 768])\n",
            "Hidden States:  torch.Size([2, 9, 768])\n",
            "Q.size torch.Size([2, 9, 768])\n",
            "K.size torch.Size([2, 9, 768])\n",
            "V.size torch.Size([2, 9, 768])\n",
            "Q.size torch.Size([2, 12, 9, 64])\n",
            "K.size torch.Size([2, 12, 9, 64])\n",
            "V.size torch.Size([2, 12, 9, 64])\n",
            "dk 64\n",
            "transpose k torch.Size([2, 12, 64, 9])\n",
            "attention score:  torch.Size([2, 12, 9, 9])\n",
            "softmax attention score:  torch.Size([2, 12, 9, 9])\n",
            "score*v torch.Size([2, 12, 9, 64])\n",
            "permute output torch.Size([2, 9, 12, 64])\n",
            "reshape output:  torch.Size([2, 9, 768])\n",
            "attention_output torch.Size([2, 9, 768])\n",
            "hidden_states(input) torch.Size([2, 9, 768])\n",
            "linear output torch.Size([2, 9, 3072])\n",
            "activate output torch.Size([2, 9, 3072])\n",
            "dropout output torch.Size([2, 9, 3072])\n",
            "feedforward_output torch.Size([2, 9, 768])\n",
            "Hidden States:  torch.Size([2, 9, 768])\n",
            "Q.size torch.Size([2, 9, 768])\n",
            "K.size torch.Size([2, 9, 768])\n",
            "V.size torch.Size([2, 9, 768])\n",
            "Q.size torch.Size([2, 12, 9, 64])\n",
            "K.size torch.Size([2, 12, 9, 64])\n",
            "V.size torch.Size([2, 12, 9, 64])\n",
            "dk 64\n",
            "transpose k torch.Size([2, 12, 64, 9])\n",
            "attention score:  torch.Size([2, 12, 9, 9])\n",
            "softmax attention score:  torch.Size([2, 12, 9, 9])\n",
            "score*v torch.Size([2, 12, 9, 64])\n",
            "permute output torch.Size([2, 9, 12, 64])\n",
            "reshape output:  torch.Size([2, 9, 768])\n",
            "attention_output torch.Size([2, 9, 768])\n",
            "hidden_states(input) torch.Size([2, 9, 768])\n",
            "linear output torch.Size([2, 9, 3072])\n",
            "activate output torch.Size([2, 9, 3072])\n",
            "dropout output torch.Size([2, 9, 3072])\n",
            "feedforward_output torch.Size([2, 9, 768])\n",
            "Hidden States:  torch.Size([2, 9, 768])\n",
            "Q.size torch.Size([2, 9, 768])\n",
            "K.size torch.Size([2, 9, 768])\n",
            "V.size torch.Size([2, 9, 768])\n",
            "Q.size torch.Size([2, 12, 9, 64])\n",
            "K.size torch.Size([2, 12, 9, 64])\n",
            "V.size torch.Size([2, 12, 9, 64])\n",
            "dk 64\n",
            "transpose k torch.Size([2, 12, 64, 9])\n",
            "attention score:  torch.Size([2, 12, 9, 9])\n",
            "softmax attention score:  torch.Size([2, 12, 9, 9])\n",
            "score*v torch.Size([2, 12, 9, 64])\n",
            "permute output torch.Size([2, 9, 12, 64])\n",
            "reshape output:  torch.Size([2, 9, 768])\n",
            "attention_output torch.Size([2, 9, 768])\n",
            "hidden_states(input) torch.Size([2, 9, 768])\n",
            "linear output torch.Size([2, 9, 3072])\n",
            "activate output torch.Size([2, 9, 3072])\n",
            "dropout output torch.Size([2, 9, 3072])\n",
            "feedforward_output torch.Size([2, 9, 768])\n",
            "Hidden States:  torch.Size([2, 9, 768])\n",
            "Q.size torch.Size([2, 9, 768])\n",
            "K.size torch.Size([2, 9, 768])\n",
            "V.size torch.Size([2, 9, 768])\n",
            "Q.size torch.Size([2, 12, 9, 64])\n",
            "K.size torch.Size([2, 12, 9, 64])\n",
            "V.size torch.Size([2, 12, 9, 64])\n",
            "dk 64\n",
            "transpose k torch.Size([2, 12, 64, 9])\n",
            "attention score:  torch.Size([2, 12, 9, 9])\n",
            "softmax attention score:  torch.Size([2, 12, 9, 9])\n",
            "score*v torch.Size([2, 12, 9, 64])\n",
            "permute output torch.Size([2, 9, 12, 64])\n",
            "reshape output:  torch.Size([2, 9, 768])\n",
            "attention_output torch.Size([2, 9, 768])\n",
            "hidden_states(input) torch.Size([2, 9, 768])\n",
            "linear output torch.Size([2, 9, 3072])\n",
            "activate output torch.Size([2, 9, 3072])\n",
            "dropout output torch.Size([2, 9, 3072])\n",
            "feedforward_output torch.Size([2, 9, 768])\n",
            "Hidden States:  torch.Size([2, 9, 768])\n",
            "Q.size torch.Size([2, 9, 768])\n",
            "K.size torch.Size([2, 9, 768])\n",
            "V.size torch.Size([2, 9, 768])\n",
            "Q.size torch.Size([2, 12, 9, 64])\n",
            "K.size torch.Size([2, 12, 9, 64])\n",
            "V.size torch.Size([2, 12, 9, 64])\n",
            "dk 64\n",
            "transpose k torch.Size([2, 12, 64, 9])\n",
            "attention score:  torch.Size([2, 12, 9, 9])\n",
            "softmax attention score:  torch.Size([2, 12, 9, 9])\n",
            "score*v torch.Size([2, 12, 9, 64])\n",
            "permute output torch.Size([2, 9, 12, 64])\n",
            "reshape output:  torch.Size([2, 9, 768])\n",
            "attention_output torch.Size([2, 9, 768])\n",
            "hidden_states(input) torch.Size([2, 9, 768])\n",
            "linear output torch.Size([2, 9, 3072])\n",
            "activate output torch.Size([2, 9, 3072])\n",
            "dropout output torch.Size([2, 9, 3072])\n",
            "feedforward_output torch.Size([2, 9, 768])\n",
            "Hidden States:  torch.Size([2, 9, 768])\n",
            "Q.size torch.Size([2, 9, 768])\n",
            "K.size torch.Size([2, 9, 768])\n",
            "V.size torch.Size([2, 9, 768])\n",
            "Q.size torch.Size([2, 12, 9, 64])\n",
            "K.size torch.Size([2, 12, 9, 64])\n",
            "V.size torch.Size([2, 12, 9, 64])\n",
            "dk 64\n",
            "transpose k torch.Size([2, 12, 64, 9])\n",
            "attention score:  torch.Size([2, 12, 9, 9])\n",
            "softmax attention score:  torch.Size([2, 12, 9, 9])\n",
            "score*v torch.Size([2, 12, 9, 64])\n",
            "permute output torch.Size([2, 9, 12, 64])\n",
            "reshape output:  torch.Size([2, 9, 768])\n",
            "attention_output torch.Size([2, 9, 768])\n",
            "hidden_states(input) torch.Size([2, 9, 768])\n",
            "linear output torch.Size([2, 9, 3072])\n",
            "activate output torch.Size([2, 9, 3072])\n",
            "dropout output torch.Size([2, 9, 3072])\n",
            "feedforward_output torch.Size([2, 9, 768])\n",
            "Hidden States:  torch.Size([2, 9, 768])\n",
            "Q.size torch.Size([2, 9, 768])\n",
            "K.size torch.Size([2, 9, 768])\n",
            "V.size torch.Size([2, 9, 768])\n",
            "Q.size torch.Size([2, 12, 9, 64])\n",
            "K.size torch.Size([2, 12, 9, 64])\n",
            "V.size torch.Size([2, 12, 9, 64])\n",
            "dk 64\n",
            "transpose k torch.Size([2, 12, 64, 9])\n",
            "attention score:  torch.Size([2, 12, 9, 9])\n",
            "softmax attention score:  torch.Size([2, 12, 9, 9])\n",
            "score*v torch.Size([2, 12, 9, 64])\n",
            "permute output torch.Size([2, 9, 12, 64])\n",
            "reshape output:  torch.Size([2, 9, 768])\n",
            "attention_output torch.Size([2, 9, 768])\n",
            "hidden_states(input) torch.Size([2, 9, 768])\n",
            "linear output torch.Size([2, 9, 3072])\n",
            "activate output torch.Size([2, 9, 3072])\n",
            "dropout output torch.Size([2, 9, 3072])\n",
            "feedforward_output torch.Size([2, 9, 768])\n",
            "Hidden States:  torch.Size([2, 9, 768])\n",
            "Q.size torch.Size([2, 9, 768])\n",
            "K.size torch.Size([2, 9, 768])\n",
            "V.size torch.Size([2, 9, 768])\n",
            "Q.size torch.Size([2, 12, 9, 64])\n",
            "K.size torch.Size([2, 12, 9, 64])\n",
            "V.size torch.Size([2, 12, 9, 64])\n",
            "dk 64\n",
            "transpose k torch.Size([2, 12, 64, 9])\n",
            "attention score:  torch.Size([2, 12, 9, 9])\n",
            "softmax attention score:  torch.Size([2, 12, 9, 9])\n",
            "score*v torch.Size([2, 12, 9, 64])\n",
            "permute output torch.Size([2, 9, 12, 64])\n",
            "reshape output:  torch.Size([2, 9, 768])\n",
            "attention_output torch.Size([2, 9, 768])\n",
            "hidden_states(input) torch.Size([2, 9, 768])\n",
            "linear output torch.Size([2, 9, 3072])\n",
            "activate output torch.Size([2, 9, 3072])\n",
            "dropout output torch.Size([2, 9, 3072])\n",
            "feedforward_output torch.Size([2, 9, 768])\n",
            "Hidden States:  torch.Size([2, 9, 768])\n",
            "Q.size torch.Size([2, 9, 768])\n",
            "K.size torch.Size([2, 9, 768])\n",
            "V.size torch.Size([2, 9, 768])\n",
            "Q.size torch.Size([2, 12, 9, 64])\n",
            "K.size torch.Size([2, 12, 9, 64])\n",
            "V.size torch.Size([2, 12, 9, 64])\n",
            "dk 64\n",
            "transpose k torch.Size([2, 12, 64, 9])\n",
            "attention score:  torch.Size([2, 12, 9, 9])\n",
            "softmax attention score:  torch.Size([2, 12, 9, 9])\n",
            "score*v torch.Size([2, 12, 9, 64])\n",
            "permute output torch.Size([2, 9, 12, 64])\n",
            "reshape output:  torch.Size([2, 9, 768])\n",
            "attention_output torch.Size([2, 9, 768])\n",
            "hidden_states(input) torch.Size([2, 9, 768])\n",
            "linear output torch.Size([2, 9, 3072])\n",
            "activate output torch.Size([2, 9, 3072])\n",
            "dropout output torch.Size([2, 9, 3072])\n",
            "feedforward_output torch.Size([2, 9, 768])\n",
            "Hidden States:  torch.Size([2, 9, 768])\n",
            "Q.size torch.Size([2, 9, 768])\n",
            "K.size torch.Size([2, 9, 768])\n",
            "V.size torch.Size([2, 9, 768])\n",
            "Q.size torch.Size([2, 12, 9, 64])\n",
            "K.size torch.Size([2, 12, 9, 64])\n",
            "V.size torch.Size([2, 12, 9, 64])\n",
            "dk 64\n",
            "transpose k torch.Size([2, 12, 64, 9])\n",
            "attention score:  torch.Size([2, 12, 9, 9])\n",
            "softmax attention score:  torch.Size([2, 12, 9, 9])\n",
            "score*v torch.Size([2, 12, 9, 64])\n",
            "permute output torch.Size([2, 9, 12, 64])\n",
            "reshape output:  torch.Size([2, 9, 768])\n",
            "attention_output torch.Size([2, 9, 768])\n",
            "hidden_states(input) torch.Size([2, 9, 768])\n",
            "linear output torch.Size([2, 9, 3072])\n",
            "activate output torch.Size([2, 9, 3072])\n",
            "dropout output torch.Size([2, 9, 3072])\n",
            "feedforward_output torch.Size([2, 9, 768])\n",
            "Hidden States:  torch.Size([2, 9, 768])\n",
            "Q.size torch.Size([2, 9, 768])\n",
            "K.size torch.Size([2, 9, 768])\n",
            "V.size torch.Size([2, 9, 768])\n",
            "Q.size torch.Size([2, 12, 9, 64])\n",
            "K.size torch.Size([2, 12, 9, 64])\n",
            "V.size torch.Size([2, 12, 9, 64])\n",
            "dk 64\n",
            "transpose k torch.Size([2, 12, 64, 9])\n",
            "attention score:  torch.Size([2, 12, 9, 9])\n",
            "softmax attention score:  torch.Size([2, 12, 9, 9])\n",
            "score*v torch.Size([2, 12, 9, 64])\n",
            "permute output torch.Size([2, 9, 12, 64])\n",
            "reshape output:  torch.Size([2, 9, 768])\n",
            "attention_output torch.Size([2, 9, 768])\n",
            "hidden_states(input) torch.Size([2, 9, 768])\n",
            "linear output torch.Size([2, 9, 3072])\n",
            "activate output torch.Size([2, 9, 3072])\n",
            "dropout output torch.Size([2, 9, 3072])\n",
            "feedforward_output torch.Size([2, 9, 768])\n",
            "[Huggingface] Output :  BaseModelOutputWithPastAndCrossAttentions(last_hidden_state=tensor([[[-0.0849, -0.7778,  0.8296,  ..., -1.1212,  0.2598, -0.4649],\n",
            "         [-0.7830, -0.2714,  1.8594,  ..., -1.4522, -0.7881, -0.3846],\n",
            "         [-1.1246,  0.0358,  1.5286,  ..., -0.5057, -0.5620, -1.1561],\n",
            "         ...,\n",
            "         [-1.0857, -0.4344,  1.2498,  ..., -0.2815, -0.9729, -0.1571],\n",
            "         [-1.0135,  0.4653,  1.0829,  ..., -1.2232, -0.5771, -0.5106],\n",
            "         [-0.5226, -0.4321,  0.2224,  ..., -0.4073, -0.2182, -1.3069]],\n",
            "\n",
            "        [[ 0.1804,  0.3630,  0.9131,  ..., -1.2643,  0.2961, -1.2234],\n",
            "         [-1.1480, -0.3254,  2.1100,  ..., -1.3642, -0.3865, -0.1745],\n",
            "         [-0.1127,  1.2578,  1.3110,  ..., -0.6124, -0.4355, -1.4773],\n",
            "         ...,\n",
            "         [-1.2414, -0.2254,  1.5572,  ...,  0.0231, -0.4564, -0.2605],\n",
            "         [-1.3822,  1.2643,  1.8318,  ..., -1.1997, -0.7393, -1.2549],\n",
            "         [-0.8797, -0.0706,  1.7029,  ..., -0.5192, -0.6313, -1.4444]]]), past_key_values=None, hidden_states=None, attentions=None, cross_attentions=None)\n",
            "[Custom] Output :  tensor([[[-0.0849, -0.7778,  0.8296,  ..., -1.1212,  0.2598, -0.4649],\n",
            "         [-0.7830, -0.2714,  1.8594,  ..., -1.4522, -0.7881, -0.3846],\n",
            "         [-1.1246,  0.0358,  1.5286,  ..., -0.5057, -0.5620, -1.1561],\n",
            "         ...,\n",
            "         [-1.0857, -0.4344,  1.2498,  ..., -0.2815, -0.9729, -0.1571],\n",
            "         [-1.0135,  0.4653,  1.0829,  ..., -1.2232, -0.5771, -0.5106],\n",
            "         [-0.5226, -0.4321,  0.2224,  ..., -0.4073, -0.2182, -1.3069]],\n",
            "\n",
            "        [[ 0.1804,  0.3630,  0.9131,  ..., -1.2643,  0.2961, -1.2234],\n",
            "         [-1.1480, -0.3254,  2.1100,  ..., -1.3642, -0.3865, -0.1745],\n",
            "         [-0.1127,  1.2578,  1.3110,  ..., -0.6124, -0.4355, -1.4773],\n",
            "         ...,\n",
            "         [-1.2414, -0.2254,  1.5572,  ...,  0.0231, -0.4564, -0.2605],\n",
            "         [-1.3822,  1.2643,  1.8318,  ..., -1.1997, -0.7393, -1.2549],\n",
            "         [-0.8797, -0.0706,  1.7029,  ..., -0.5192, -0.6313, -1.4444]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9tqzdcwVko0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5277558f-45b4-4233-b569-1ee4716cc5c4"
      },
      "source": [
        "print(torch.eq(encoder_embedding[0], layer_output_custom))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[True, True, True,  ..., True, True, True],\n",
            "         [True, True, True,  ..., True, True, True],\n",
            "         [True, True, True,  ..., True, True, True],\n",
            "         ...,\n",
            "         [True, True, True,  ..., True, True, True],\n",
            "         [True, True, True,  ..., True, True, True],\n",
            "         [True, True, True,  ..., True, True, True]],\n",
            "\n",
            "        [[True, True, True,  ..., True, True, True],\n",
            "         [True, True, True,  ..., True, True, True],\n",
            "         [True, True, True,  ..., True, True, True],\n",
            "         ...,\n",
            "         [True, True, True,  ..., True, True, True],\n",
            "         [True, True, True,  ..., True, True, True],\n",
            "         [True, True, True,  ..., True, True, True]]])\n"
          ]
        }
      ]
    }
  ]
}