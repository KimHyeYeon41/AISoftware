{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[AI_05]Multi-Head_Attention_201802083.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtKj8GXiamrq"
      },
      "source": [
        "## Colab 환경 구축\n",
        "\n",
        "\n",
        "### 활용 라이브러리 (고정)\n",
        "\n",
        "*   [torch==1.9.0](https://pytorch.org/)\n",
        "*   [transformers](https://pypi.org/project/transformers/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FsBNg79oagtH",
        "outputId": "bcc36f06-1704-4579-8fc2-f0615bdc5bb0"
      },
      "source": [
        "!pip3 install torch==1.9.0 torchvision torchaudio\n",
        "!pip3 install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.7/dist-packages (1.9.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.10.0+cu111)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.7/dist-packages (0.9.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.19.5)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.12.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.1.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQ7D1oNkySFb"
      },
      "source": [
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers import BertTokenizer, BertConfig, set_seed"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8u0JKpmI9fAh"
      },
      "source": [
        "1. Reproducibility- 재구현을 위한 random_seed 42로 설정\n",
        "\n",
        "```\n",
        "set_seed(42)\n",
        "```\n",
        "\n",
        "\n",
        "2. BERT [tokenizer](https://huggingface.co/transformers/model_doc/bert.html#berttokenizer), [configuration(config)](https://huggingface.co/transformers/model_doc/bert.html#bertconfig) 설정\n",
        "\n",
        "```\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "bert_configuraiton = BertConfig.from_pretrained('bert-base-cased')\n",
        "```\n",
        "\n",
        "\n",
        "3. input sequence 토크나이징\n",
        "\n",
        "```\n",
        "input_texts = ['I love cats!', 'He hates pineapple pizza.']\n",
        "\n",
        "input_sequences = tokenizer(text=input_texts, add_special_tokens=True, padding=True, truncation=True, return_tensors='pt')\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWyv_G8mabca",
        "outputId": "5bbd85bf-f7c3-47e1-bdfb-5dfafce21313"
      },
      "source": [
        "# Set seed for reproducibility\n",
        "set_seed(42)\n",
        "\n",
        "# Create BertTokenizer, Configuration\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "bert_configuraiton = BertConfig.from_pretrained('bert-base-cased')\n",
        "\n",
        "# Create input sequence using tokenizer\n",
        "input_texts = ['I love cats!', 'He hates pineapple pizza.']\n",
        "labels = [1, 0] # labels는 이번 실습에 사용되지 않음 (신경쓰지 말 것)\n",
        "input_sequences = tokenizer(text=input_texts, add_special_tokens=True, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "# Since input sequences is a dictionary we can also add to labels to it\n",
        "# want to make sure all values at tensors\n",
        "input_sequences.update({'labels':torch.tensor(labels)})\n",
        "print(input_sequences)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[  101,   146,  1567, 11771,   106,   102,     0,     0,     0],\n",
            "        [  101,  1124, 18457, 10194, 11478,  7136, 13473,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([1, 0])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsMw5RcoV5QO"
      },
      "source": [
        "### Input Sequence에 대해 Embedding 진행 (Huggingface 코드)\n",
        "- 변경 필요 없음, 그대로 사용\n",
        "\n",
        "1. BertEmbedding에 대한 configuration 생성\n",
        "2. bert_embedding block에 대해 forward 수행\n",
        "3. Embedding Output Shape : [batch_size, seq_len, hidden_size(768)]\n",
        "\n",
        "```\n",
        "# Create Bert embedding layer\n",
        "bert_embeddings_block = BertEmbeddings(bert_configuraiton)\n",
        "\n",
        "# Perform a forward pass\n",
        "embedding_output = bert_embeddings_block.forward(input_ids=input_sequences['input_ids'], token_type_ids=input_sequences['token_type_ids'])\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxIyqtH3a0ck",
        "outputId": "922a365f-15f3-41cd-f993-eca0a738e92b"
      },
      "source": [
        "class BertEmbeddings(nn.Module):\n",
        "    \"\"\"Construct the embeddings from word, position and token_type embeddings.\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n",
        "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
        "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
        "\n",
        "        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n",
        "        # any TensorFlow checkpoint file\n",
        "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n",
        "        self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))\n",
        "        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
        "\n",
        "    def forward(\n",
        "        self, input_ids=None, token_type_ids=None, position_ids=None, inputs_embeds=None, past_key_values_length=0\n",
        "    ):\n",
        "        # print('============== BertEmbeddings ==============')\n",
        "        if input_ids is not None:\n",
        "            input_shape = input_ids.size()\n",
        "        else:\n",
        "            input_shape = inputs_embeds.size()[:-1]\n",
        "\n",
        "        seq_length = input_shape[1]\n",
        "\n",
        "        if position_ids is None:\n",
        "            position_ids = self.position_ids[:, past_key_values_length : seq_length + past_key_values_length]       \n",
        "        #print('Created Tokens Positions IDs: ', position_ids) # ADDED\n",
        "        \n",
        "\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n",
        "\n",
        "        if inputs_embeds is None:\n",
        "            inputs_embeds = self.word_embeddings(input_ids)\n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
        "\n",
        "        # ADDED\n",
        "        print('Tokens IDs: ', input_ids.shape)\n",
        "        print('Tokens Type IDs: ', token_type_ids.shape)\n",
        "        print('Word Embeddings: ', inputs_embeds.shape)\n",
        "\n",
        "        embeddings = inputs_embeds + token_type_embeddings\n",
        "        if self.position_embedding_type == \"absolute\":\n",
        "            position_embeddings = self.position_embeddings(position_ids)\n",
        "            # print('Position Embeddings: ', position_embeddings.shape) # ADDED\n",
        "\n",
        "            embeddings += position_embeddings\n",
        "\n",
        "        # ADDED\n",
        "        # print('Token Types Embeddings: ', token_type_embeddings.shape)\n",
        "        # print('Sum Up All Embeddings: ', embeddings.shape)\n",
        "\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        # print('Embeddings Layer Nromalization: ', embeddings.shape) # ADDED\n",
        "\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        # print('Embeddings Dropout Layer: ', embeddings.shape) # ADDED\n",
        "        \n",
        "        return embeddings\n",
        "\n",
        "# Create Bert embedding layer\n",
        "bert_embeddings_block = BertEmbeddings(bert_configuraiton)\n",
        "\n",
        "# Perform a forward pass\n",
        "embedding_output = bert_embeddings_block.forward(input_ids=input_sequences['input_ids'], token_type_ids=input_sequences['token_type_ids'])\n",
        "print('Embedding Output: ', embedding_output.shape) # ADDED"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens IDs:  torch.Size([2, 9])\n",
            "Tokens Type IDs:  torch.Size([2, 9])\n",
            "Word Embeddings:  torch.Size([2, 9, 768])\n",
            "Embedding Output:  torch.Size([2, 9, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDx5L984WctE"
      },
      "source": [
        "### Self-Attention 수행 (Huggingface 코드)\n",
        "\n",
        "- 결과로 나오는 output shape : [batch_size, seq_len, 768] "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYtHqaaqbJop"
      },
      "source": [
        "class BertSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n",
        "            raise ValueError(\n",
        "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
        "                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads)\n",
        "            )\n",
        "\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        # ADDED\n",
        "        # print('============== BertSelfAttention ==============')\n",
        "        # print('Attention Head Size: ', self.attention_head_size)\n",
        "        # print('Combined Attentions Head Size: ', self.all_head_size)\n",
        "\n",
        "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
        "        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
        "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
        "            self.max_position_embeddings = config.max_position_embeddings\n",
        "            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n",
        "\n",
        "        self.is_decoder = config.is_decoder\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        past_key_value=None,\n",
        "        output_attentions=False,\n",
        "    ):\n",
        "        \n",
        "        print('Hidden States: ', hidden_states.shape) # ADDED\n",
        "\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "\n",
        "        # If this is instantiated as a cross-attention module, the keys\n",
        "        # and values come from an encoder; the attention mask needs to be\n",
        "        # such that the encoder's padding tokens are not attended to.\n",
        "        is_cross_attention = encoder_hidden_states is not None\n",
        "\n",
        "        if is_cross_attention and past_key_value is not None:\n",
        "            # ADDED\n",
        "            # print('Query Linear Layer: ', mixed_query_layer.shape)\n",
        "            # print('Key Linear Layer: ', past_key_value[0].shape)\n",
        "            # print('Value Linear Layer: ', past_key_value[1].shape)\n",
        "\n",
        "            # reuse k,v, cross_attentions\n",
        "            key_layer = past_key_value[0]\n",
        "            value_layer = past_key_value[1]\n",
        "            attention_mask = encoder_attention_mask\n",
        "        \n",
        "        elif is_cross_attention:\n",
        "            # ADDED\n",
        "            # print('Query Linear Layer: ', mixed_query_layer.shape)\n",
        "            # print('Key Linear Layer: ', self.key(encoder_hidden_states).shape)\n",
        "            # print('Value Linear Layer: ', self.value(encoder_hidden_states).shape)\n",
        "\n",
        "            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n",
        "            attention_mask = encoder_attention_mask\n",
        "        \n",
        "        elif past_key_value is not None:\n",
        "            # ADDED\n",
        "            # print('Query Linear Layer: ', mixed_query_layer.shape)\n",
        "            # print('Key Linear Layer: ', self.key(hidden_states).shape)\n",
        "            # print('Value Linear Layer: ', self.value(hidden_states).shape)\n",
        "\n",
        "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n",
        "            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n",
        "        \n",
        "        else:\n",
        "            # ADDED\n",
        "            # print('Query Linear Layer: ', mixed_query_layer.shape)\n",
        "            # print('Key Linear Layer: ', self.key(hidden_states).shape)\n",
        "            # print('Value Linear Layer: ', self.value(hidden_states).shape)\n",
        "\n",
        "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "\n",
        "        # ADDED\n",
        "        # print('Query: ', query_layer.shape)\n",
        "        # print('Key: ', key_layer.shape)\n",
        "        # print('Value: ', value_layer.shape)\n",
        "\n",
        "        if self.is_decoder:\n",
        "            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n",
        "            # Further calls to cross_attention layer can then reuse all cross-attention\n",
        "            # key/value_states (first \"if\" case)\n",
        "            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n",
        "            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n",
        "            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n",
        "            # if encoder bi-directional self-attention `past_key_value` is always `None`\n",
        "            past_key_value = (key_layer, value_layer)\n",
        "\n",
        "        # ADDED\n",
        "        # print('Key Transposed: ', key_layer.transpose(-1, -2).shape)\n",
        "\n",
        "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "\n",
        "        # ADDED\n",
        "        # print('Attention Scores: ', attention_scores.shape)\n",
        "\n",
        "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
        "            seq_length = hidden_states.size()[1]\n",
        "            position_ids_l = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n",
        "            position_ids_r = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n",
        "            distance = position_ids_l - position_ids_r\n",
        "            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n",
        "            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n",
        "\n",
        "            if self.position_embedding_type == \"relative_key\":\n",
        "                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
        "                attention_scores = attention_scores + relative_position_scores\n",
        "            elif self.position_embedding_type == \"relative_key_query\":\n",
        "                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
        "                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n",
        "                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n",
        "\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size) # root 계산\n",
        "        # print('Attention Scores Divided by Scalar: ', attention_scores.shape) # ADDED\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
        "            attention_scores = attention_scores + attention_mask\n",
        "\n",
        "        # Normalize the attention scores to probabilities.\n",
        "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
        "        # print('Attention Probabilities Softmax Layer: ', attention_probs.shape) # ADDED\n",
        "\n",
        "        # This is actually dropping out entire tokens to attend to, which might\n",
        "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "        # print('Attention Probabilities Dropout Layer: ', attention_probs.shape) # ADDED\n",
        "\n",
        "        # Mask heads if we want to\n",
        "        if head_mask is not None:\n",
        "            attention_probs = attention_probs * head_mask\n",
        "\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "        # print('Context: ', context_layer.shape) # ADDED\n",
        "\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        # print('Context Permute: ', context_layer.shape) # ADDED\n",
        "\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(*new_context_layer_shape)\n",
        "        # print('Context Reshaped: ', context_layer.shape) # ADDED\n",
        "        \n",
        "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
        "\n",
        "        if self.is_decoder:\n",
        "            outputs = outputs + (past_key_value,)\n",
        "        return outputs"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGKxSrdjb41v"
      },
      "source": [
        "def get_extended_attention_mask(attention_mask, input_shape, device):\n",
        "        \"\"\"\n",
        "        Makes broadcastable attention and causal masks so that future and masked tokens are ignored.\n",
        "        Arguments:\n",
        "            attention_mask (:obj:`torch.Tensor`):\n",
        "                Mask with ones indicating tokens to attend to, zeros for tokens to ignore.\n",
        "            input_shape (:obj:`Tuple[int]`):\n",
        "                The shape of the input to the model.\n",
        "            device: (:obj:`torch.device`):\n",
        "                The device of the input to the model.\n",
        "        Returns:\n",
        "            :obj:`torch.Tensor` The extended attention mask, with a the same dtype as :obj:`attention_mask.dtype`.\n",
        "        \"\"\"\n",
        "        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
        "        # ourselves in which case we just need to make it broadcastable to all heads.\n",
        "        if attention_mask.dim() == 3:\n",
        "            extended_attention_mask = attention_mask[:, None, :, :]\n",
        "        elif attention_mask.dim() == 2:\n",
        "            # Provided a padding mask of dimensions [batch_size, seq_length]\n",
        "            # - if the model is a decoder, apply a causal mask in addition to the padding mask\n",
        "            # - if the model is an encoder, make the mask broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
        "            \n",
        "            extended_attention_mask = attention_mask[:, None, None, :]\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"Wrong shape for input_ids (shape {input_shape}) or attention_mask (shape {attention_mask.shape})\"\n",
        "            )\n",
        "\n",
        "        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
        "        # masked positions, this operation will create a tensor which is 0.0 for\n",
        "        # positions we want to attend and -10000.0 for masked positions.\n",
        "        # Since we are adding it to the raw scores before the softmax, this is\n",
        "        # effectively the same as removing these entirely.\n",
        "        \n",
        "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
        "        return extended_attention_mask.to(device)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIDXmTE1W1hi"
      },
      "source": [
        "### Custom Multi-Head Attention\n",
        "\n",
        "- 직접 구현해야 하는 부분\n",
        "- 결과로 나오는 output shape : [batch_size, seq_len, 768] "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9crjwvrMb14X"
      },
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, hid_dim, n_heads):\n",
        "        super().__init__()\n",
        "        \n",
        "        assert hid_dim % n_heads == 0\n",
        "        \n",
        "        self.hid_dim = hid_dim \n",
        "        self.n_heads = n_heads \n",
        "        self.head_dim = hid_dim // n_heads \n",
        "        \n",
        "        self.query = nn.Linear(hid_dim, hid_dim)\n",
        "        self.key = nn.Linear(hid_dim, hid_dim)\n",
        "        self.value = nn.Linear(hid_dim, hid_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        \n",
        "        \n",
        "    def forward(self, hidden_states, attention_mask=None):\n",
        "      \n",
        "        print('Hidden States: ', hidden_states.shape) # ADDED\n",
        "\n",
        "        Q = self.query(hidden_states)\n",
        "        K = self.key(hidden_states)\n",
        "        V = self.value(hidden_states)\n",
        "        print('Q.size',Q.size())\n",
        "        print('K.size',K.size())\n",
        "        print('V.size',V.size())\n",
        "       \n",
        "        batch_size = hidden_states.shape[0]\n",
        "               \n",
        "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0,2,1,3)\n",
        "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0,2,1,3)\n",
        "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0,2,1,3)\n",
        "        print('Q.size',Q.size())\n",
        "        print('K.size',K.size())\n",
        "        print('V.size',V.size())\n",
        "        \n",
        "        d_k = self.head_dim # d_k\n",
        "        print('dk',d_k)\n",
        "        print('transpose k', K.transpose(-2,-1).size())\n",
        "        attention_score = torch.matmul(Q, K.transpose(-1,-2)) # Q x K^T\n",
        "        attention_score = attention_score / math.sqrt(d_k) \n",
        "        print('attention score: ', attention_score.size())\n",
        "        \n",
        "        if attention_mask is not None:\n",
        "          attention_score = attention_score + attention_mask\n",
        "        \n",
        "        attention = nn.functional.softmax(attention_score, dim=-1) \n",
        "        print('softmax attention score: ', attention.size())\n",
        "        \n",
        "        attention = self.dropout(attention)\n",
        "        \n",
        "        output = torch.matmul(attention,V) \n",
        "        print('score*v',output.size())\n",
        "\n",
        "        output = output.permute(0, 2, 1, 3) \n",
        "        print('permute output',output.size())\n",
        "\n",
        "        output = output.reshape(2,9,768)\n",
        "        print('reshape output: ', output.size())\n",
        "\n",
        "        return output\n",
        "        "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5du20dkXEVn"
      },
      "source": [
        "- Attention Layer 선언\n",
        "- Huggingface BertSelfAttention 파라미터를 custom attention으로 가져옴\n",
        "- Randomness 최소화\n",
        "  - eval() 모드\n",
        "  - with torch.no_grad() \n",
        "\n",
        "- 두 개의 Attention Layer로부터 Forward 결과로 나오는 output 값 비교"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2ioE8oQteL7",
        "outputId": "c96f7a2e-880a-44d2-e1b1-fd5b1918d1b3"
      },
      "source": [
        "# Create bert self attention layer\n",
        "bert_selfattention_block_huggingface = BertSelfAttention(bert_configuraiton)\n",
        "bert_selfattention_block = MultiHeadAttention(hid_dim=768, n_heads=12) # <-- custom attention\n",
        "\n",
        "# huggingface attention의 parameter 가져옴\n",
        "bert_selfattention_block.query.load_state_dict( bert_selfattention_block_huggingface.query.state_dict())\n",
        "bert_selfattention_block.key.load_state_dict( bert_selfattention_block_huggingface.key.state_dict())\n",
        "bert_selfattention_block.value.load_state_dict( bert_selfattention_block_huggingface.value.state_dict())\n",
        "\n",
        "# eval mode 설정\n",
        "bert_selfattention_block_huggingface.eval()\n",
        "bert_selfattention_block.eval() "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultiHeadAttention(\n",
              "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPXvJJCpp1DB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d153fec7-9a6d-4b83-c0fe-ea4260ba873f"
      },
      "source": [
        "# Perform a forward pass\n",
        "with torch.no_grad():\n",
        "    input_shape= input_sequences.input_ids.shape\n",
        "    attention_mask = get_extended_attention_mask(input_sequences.attention_mask, input_shape, input_sequences.input_ids.device)\n",
        "    \n",
        "    context_embedding_huggingface = bert_selfattention_block_huggingface.forward(hidden_states=embedding_output, attention_mask=attention_mask)\n",
        "    context_embedding_custom = bert_selfattention_block.forward(hidden_states=embedding_output, attention_mask=attention_mask)\n",
        "\n",
        "print('[Huggingface] Context Embedding : ', context_embedding_huggingface[0])\n",
        "print('[Custom] Context Embedding : ', context_embedding_custom)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hidden States:  torch.Size([2, 9, 768])\n",
            "Hidden States:  torch.Size([2, 9, 768])\n",
            "Q.size torch.Size([2, 9, 768])\n",
            "K.size torch.Size([2, 9, 768])\n",
            "V.size torch.Size([2, 9, 768])\n",
            "Q.size torch.Size([2, 12, 9, 64])\n",
            "K.size torch.Size([2, 12, 9, 64])\n",
            "V.size torch.Size([2, 12, 9, 64])\n",
            "dk 64\n",
            "transpose k torch.Size([2, 12, 64, 9])\n",
            "attention score:  torch.Size([2, 12, 9, 9])\n",
            "softmax attention score:  torch.Size([2, 12, 9, 9])\n",
            "score*v torch.Size([2, 12, 9, 64])\n",
            "permute output torch.Size([2, 9, 12, 64])\n",
            "reshape output:  torch.Size([2, 9, 768])\n",
            "[Huggingface] Context Embedding :  tensor([[[-0.0682,  0.2734,  0.0319,  ..., -0.1064, -0.0046,  0.1235],\n",
            "         [ 0.0223,  0.3108,  0.0203,  ..., -0.0182, -0.0749,  0.1181],\n",
            "         [ 0.0030,  0.2641,  0.0293,  ..., -0.0758, -0.0325,  0.1213],\n",
            "         ...,\n",
            "         [ 0.0562,  0.3384,  0.0308,  ..., -0.1732, -0.0441,  0.1613],\n",
            "         [ 0.0049,  0.3068,  0.0321,  ..., -0.0700, -0.0311,  0.1123],\n",
            "         [ 0.0239,  0.3569,  0.0377,  ..., -0.0650, -0.0229,  0.1237]],\n",
            "\n",
            "        [[ 0.1852,  0.3194,  0.1077,  ...,  0.0112,  0.3118,  0.1597],\n",
            "         [ 0.2177,  0.3572,  0.1157,  ...,  0.0450,  0.1716,  0.0819],\n",
            "         [ 0.1233,  0.3283,  0.0834,  ...,  0.0220,  0.2430,  0.1432],\n",
            "         ...,\n",
            "         [ 0.1253,  0.2915,  0.0154,  ..., -0.0006,  0.3482,  0.1542],\n",
            "         [ 0.1619,  0.3558,  0.0363,  ...,  0.0033,  0.2701,  0.1502],\n",
            "         [ 0.2292,  0.3222,  0.1622,  ...,  0.0264,  0.2464,  0.0891]]])\n",
            "[Custom] Context Embedding :  tensor([[[-0.0682,  0.2734,  0.0319,  ..., -0.1064, -0.0046,  0.1235],\n",
            "         [ 0.0223,  0.3108,  0.0203,  ..., -0.0182, -0.0749,  0.1181],\n",
            "         [ 0.0030,  0.2641,  0.0293,  ..., -0.0758, -0.0325,  0.1213],\n",
            "         ...,\n",
            "         [ 0.0562,  0.3384,  0.0308,  ..., -0.1732, -0.0441,  0.1613],\n",
            "         [ 0.0049,  0.3068,  0.0321,  ..., -0.0700, -0.0311,  0.1123],\n",
            "         [ 0.0239,  0.3569,  0.0377,  ..., -0.0650, -0.0229,  0.1237]],\n",
            "\n",
            "        [[ 0.1852,  0.3194,  0.1077,  ...,  0.0112,  0.3118,  0.1597],\n",
            "         [ 0.2177,  0.3572,  0.1157,  ...,  0.0450,  0.1716,  0.0819],\n",
            "         [ 0.1233,  0.3283,  0.0834,  ...,  0.0220,  0.2430,  0.1432],\n",
            "         ...,\n",
            "         [ 0.1253,  0.2915,  0.0154,  ..., -0.0006,  0.3482,  0.1542],\n",
            "         [ 0.1619,  0.3558,  0.0363,  ...,  0.0033,  0.2701,  0.1502],\n",
            "         [ 0.2292,  0.3222,  0.1622,  ...,  0.0264,  0.2464,  0.0891]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4HATdEBs7Pa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31be36a3-abf7-4f8d-cf35-55e86e2a953b"
      },
      "source": [
        "print(torch.eq(context_embedding_huggingface[0], context_embedding_custom))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[True, True, True,  ..., True, True, True],\n",
            "         [True, True, True,  ..., True, True, True],\n",
            "         [True, True, True,  ..., True, True, True],\n",
            "         ...,\n",
            "         [True, True, True,  ..., True, True, True],\n",
            "         [True, True, True,  ..., True, True, True],\n",
            "         [True, True, True,  ..., True, True, True]],\n",
            "\n",
            "        [[True, True, True,  ..., True, True, True],\n",
            "         [True, True, True,  ..., True, True, True],\n",
            "         [True, True, True,  ..., True, True, True],\n",
            "         ...,\n",
            "         [True, True, True,  ..., True, True, True],\n",
            "         [True, True, True,  ..., True, True, True],\n",
            "         [True, True, True,  ..., True, True, True]]])\n"
          ]
        }
      ]
    }
  ]
}